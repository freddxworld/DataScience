{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9932359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential# Model type to be used\n",
    "##allows to the creation of a deep learning model by adding layers to it, here every unit\n",
    "#in a layer is connected to every unit in the prvious layer\n",
    "from keras.layers.core import Dense, Dropout, Activation \n",
    "from keras.utils import np_utils \n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27badd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the csv file\n",
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03d0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into input and output\n",
    "x = df[['X1', 'X2', 'X3', 'X4', 'X5']]\n",
    "y = df[['Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4410b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the taining and testing data using the 80/20 rule\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e846376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape to vectors\n",
    "#np.set_printoptions(suppress=True)\n",
    "#x_train=np.array(x_train)\n",
    "#converting our object types from float64 to float 32\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "#normalizing data which is really good to combat against overfitting\n",
    "x_train /= 255\n",
    "#225\n",
    "x_test /= 255\n",
    "#225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8d54203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting our input values into binary so that our NN can use the input\n",
    "#we are using 5 because that is the amount of differnt values each input has\n",
    "nb_classes = 5\n",
    "#we are then adding the binary values to the output data\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cbdfca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 500)               3000      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 500)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 500)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                25050     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 50)                0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 25)                1275      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 25)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 25)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 130       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,455\n",
      "Trainable params: 29,455\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#this is our first hidden layer which has 500 neurons and input of 5 vectors\n",
    "model.add(Dense(500, input_shape= (5,)))\n",
    "#we then add an activation fucntion, and for it we choose the relu cause its most commonly\n",
    "#used for multilayer neurnal networks\n",
    "model.add(Activation('relu'))\n",
    "#we also add a dropout which helps combat against overfitting\n",
    "model.add(Dropout(0.2))\n",
    "#then we reach the 2nd hidden layer which has 50 neurons\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "#we then add one more last hidden layer which has 25 neurons\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "# Then we have our final layer which outputs 5 classes\n",
    "model.add(Dense(5))\n",
    "#the softmax function converts binary values into classes\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9d5e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we then compile our model using the cross entropy loss fucntion \n",
    "#we also include the adam optimizer which is basiclly subsitue gradient descent for traning deep learning models\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fbb00af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "51/51 [==============================] - 2s 5ms/step - loss: 0.6094 - accuracy: 0.8400\n",
      "Epoch 2/10\n",
      "51/51 [==============================] - 0s 5ms/step - loss: 0.1409 - accuracy: 0.9469\n",
      "Epoch 3/10\n",
      "51/51 [==============================] - 0s 5ms/step - loss: 0.0928 - accuracy: 0.9625\n",
      "Epoch 4/10\n",
      "51/51 [==============================] - 0s 5ms/step - loss: 0.0680 - accuracy: 0.9804\n",
      "Epoch 5/10\n",
      "51/51 [==============================] - 0s 5ms/step - loss: 0.0596 - accuracy: 0.9850\n",
      "Epoch 6/10\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.0580 - accuracy: 0.9846\n",
      "Epoch 7/10\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.0552 - accuracy: 0.9863\n",
      "Epoch 8/10\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.0537 - accuracy: 0.9851\n",
      "Epoch 9/10\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.0526 - accuracy: 0.9866\n",
      "Epoch 10/10\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.0527 - accuracy: 0.9868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22d0bcaeb50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting our model and including a batch size of 128, verbose = 1 which shows us how the model is running and epoch which\n",
    "#is at 6 so our model will be trained with batches 6 times\n",
    "model.fit(x_train, Y_train, batch_size = 128, epochs = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b72e039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 1s 2ms/step - loss: 0.0427 - accuracy: 0.9890\n",
      "Test loss: 0.04274677112698555\n",
      "Test accuracy: 0.988950252532959\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the test data\n",
    "score = model.evaluate(x_test, Y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96b545cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on the accuracy for both of the training data and testing data we can see that the model is not overfitting\n",
    "#it also not underfitting\n",
    "#During the project i really didnt come accros the data underfitting or overfitting, mainly because on the first run\n",
    "#I normalzied the data and i also made sure to add drops in my model which helps combat against overfitting\n",
    "#in the end i was able to reach accuracy 0.99 however i wasnt able to increase anymore from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19d9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc20b4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
